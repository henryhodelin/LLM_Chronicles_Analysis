---
layout: default
---

# LLM Chronicles Analysis

<div style="position: fixed; top: 10px; right: 10px;">
    <a href="https://henryhodelin.github.io/Short_Resume_EN/">
        <button type="button">Spanish</button>
    </a>
</div>

## Neural Networks and Multi-Layer Perceptrons 

- Why is important to understand the neural networks ?
- What are the neural networks ?
  - Perceptrons and artificial neurons
  - Multilayer Perceptrons
- Why use neural networks ?
  - Concept of function
  - Functionals and operators
  - Universal aproximators
- Modeling inputs for a neural networks 
- Classification and Regression Tasks
- Tensors and GPUs
- Processing data in Batches


## Multi-Layer Perceptrons and MNIST Digit Classification and Nonlinear Regression using PyTorch and Tensorflow

- Loss function and gradient descent
- Mini-batch updates, momentum, SGD, ADAM
- Evaluation, overfitting and underfitting
- Digital recognition implementation
- Regresion implementation


## Recurrent Neural Networks for Modelling Sequential Data

- RNN and LSTM Cells
- Language Modelling 
- Building a World-Level Language Model using RNNs
-Encoder/ Decoder RNN for Language Translation
- Attention Mechanism

## The Transformer Architecture

- Making LLMs from Transformers: BERT
- Fine-tuning DistilBERT 
- GPT, Instruction Fine-Tuning, RLHF
- RAG (Retrieval Augmented Generation)



[.                         .NEXT](./Neural Networks_and_Multi-Layer_Perceptrons.html)

